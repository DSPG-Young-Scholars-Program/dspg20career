---
title: "15b_weighted_cluster_tests"
author: "Joanna Schroeder"
date: "9/4/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
for (pkg in c("tidyverse",  "data.table", "R.utils", "maditr", "stringr", "stringi", "dplyr", "ggplot2", "lubridate", "gt", "DataExplorer", "TraMineR", "TraMineRextras", "cluster", "tools", "WeightedCluster")) {
  library(pkg, character.only = TRUE)
}

uva_color_palette <- 
c("#232D4B", #space cadet
  "#2C4F6B", #indigo dye
  "#0E879C", #blue munsell
  "#60999A", #cadet blue
  "#D1E0BF", #tea green
  "#D9E12B", #pear
  "#E6CE3A", #citrine
  "#E6A01D", #marigold
  "#E57200" #princeton orange
)
```

How does time to run steps of a sequence clustering scale? Two main steps:

1. Calculating a distance matrix
2. Implementing clustering

We will test this using sequence objects with 1,000, 2,000, and 4,000 members. 

## Create sequence objects
```{r}
set.seed(09042020)

sts_nonvet <- read.csv("~/git/DSPG2020/career/data/sts_all_ten.csv", row.names = 1)

small <- sts_nonvet[sample(nrow(sts_nonvet), 1000, replace = FALSE, prob = NULL),]
medium <- sts_nonvet[sample(nrow(sts_nonvet), 2000, replace = FALSE, prob = NULL),]
large <- sts_nonvet[sample(nrow(sts_nonvet), 4000, replace = FALSE, prob = NULL),]

small <- seqdef(small, left="DEL", gaps="Civilian unemployment", right="DEL", cpal = c("#CBBEB5",  uva_color_palette[1], uva_color_palette[2],uva_color_palette[4],  uva_color_palette[6], uva_color_palette[9], "#CBBEB5"))

medium <- seqdef(medium, left="DEL", gaps="Civilian unemployment", right="DEL", cpal = c("#CBBEB5",  uva_color_palette[1], uva_color_palette[2],uva_color_palette[4],  uva_color_palette[6], uva_color_palette[9], "#CBBEB5"))

large <- seqdef(large, left="DEL", gaps="Civilian unemployment", right="DEL", cpal = c("#CBBEB5",  uva_color_palette[1], uva_color_palette[2],uva_color_palette[4],  uva_color_palette[6], uva_color_palette[9], "#CBBEB5"))



```


## 2,000 member clustering

```{r}
# Normal clustering

reps = 1:10

cluster_times <- for(rep in reps) {
rep = as.character(rep)
  
start_n <- Sys.time()
diss <- seqdist(small, method = "OM", indel = 1, sm = "TRATE", with.missing = TRUE)
clusterward <- agnes(diss, diss = TRUE, method = "ward")
end_n <- Sys.time()
assign(paste0("n_run_", rep), (end_n - start_n), envir = .GlobalEnv)

# Weighted clustering
start_w <- Sys.time()
agg_small <- wcAggregateCases(small)

unique_small <- small[agg_small$aggIndex,]

small.seq <- seqdef(unique_small, weights = agg_small$aggWeights)
diss <- seqdist(small.seq, method = "OM", indel = 1, sm = "TRATE", with.missing = TRUE)
clusterward <- agnes(diss, diss = TRUE, method = "ward")
#clusterward <- hclust(as.dist(diss), method = "ward.D2", members = agg_sts_all_thirty$aggWeights)
end_w <- Sys.time()
assign(paste0("w_run_", rep), (end_w - start_w), envir = .GlobalEnv)
}

small_n_time <- mean.difftime((n_run_1 + n_run_2 + n_run_3 + n_run_4 + n_run_5 + n_run_6 + n_run_7 + n_run_8 + n_run_9 + n_run_10)/10)

small_w_time <- mean.difftime((w_run_1 + w_run_2 + w_run_3 + w_run_4 + w_run_5 + w_run_6 + w_run_7 + w_run_8 + w_run_9 + w_run_10)/10)

small_n_time
small_w_time
```

## 4,000 member clustering
```{r}
# Normal clustering

reps = 1:5

cluster_times <- for(rep in reps) {
rep = as.character(rep)
  
start_n <- Sys.time()
diss <- seqdist(medium, method = "OM", indel = 1, sm = "TRATE", with.missing = TRUE)
clusterward <- agnes(diss, diss = TRUE, method = "ward")
end_n <- Sys.time()
assign(paste0("n_run_", rep), (end_n - start_n), envir = .GlobalEnv)

# Weighted clustering
start_w <- Sys.time()
agg_medium <- wcAggregateCases(medium)

unique_medium <- medium[agg_medium$aggIndex,]

medium.seq <- seqdef(unique_medium, weights = agg_medium$aggWeights)
diss <- seqdist(medium.seq, method = "OM", indel = 1, sm = "TRATE", with.missing = TRUE)
clusterward <- agnes(diss, diss = TRUE, method = "ward")
#clusterward <- hclust(as.dist(diss), method = "ward.D2", members = agg_sts_all_thirty$aggWeights)
end_w <- Sys.time()
assign(paste0("w_run_", rep), (end_w - start_w), envir = .GlobalEnv)
}

medium_n_time <- mean.difftime((n_run_1 + n_run_2 + n_run_3 + n_run_4 + n_run_5)/5)

medium_w_time <- mean.difftime((w_run_1 + w_run_2 + w_run_3 + w_run_4 + w_run_5)/5)

medium_n_time
medium_w_time
```


## 4,000 member clustering
```{r}
# Normal clustering

reps = 2

cluster_times <- for(rep in reps) {
rep = as.character(rep)
  
start_n <- Sys.time()
diss <- seqdist(large, method = "OM", indel = 1, sm = "TRATE", with.missing = TRUE)
clusterward_n <- agnes(diss, diss = TRUE, method = "ward")
end_n <- Sys.time()
assign(paste0("n_run_", rep), (end_n - start_n), envir = .GlobalEnv)

# Weighted clustering
start_w <- Sys.time()
agg_large <- wcAggregateCases(large)

unique_large <- large[agg_large$aggIndex,]

large.seq <- seqdef(unique_large, weights = agg_large$aggWeights)
diss <- seqdist(large.seq, method = "OM", indel = 1, sm = "TRATE", with.missing = TRUE)
clusterward_w <- agnes(diss, diss = TRUE, method = "ward")
#clusterward <- hclust(as.dist(diss), method = "ward.D2", members = agg_sts_all_thirty$aggWeights)
end_w <- Sys.time()
assign(paste0("w_run_", rep), (end_w - start_w), envir = .GlobalEnv)
assign("clusterward_n", clusterward_n, envir = .GlobalEnv)
assign("clusterward_w", clusterward_w, envir = .GlobalEnv)
}

large_n_time <- mean.difftime((n_run_1 + n_run_2)/2)

large_w_time <- mean.difftime((w_run_1 + w_run_2)/2)

large_n_time
large_w_time
```

## Time table
```{r}
times <- tibble("type" = c("unweighted", "weighted"),
                "1000" = c(small_n_time, small_w_time),
                "2000" = c(medium_n_time, medium_w_time),
                "4000" = c(large_n_time, large_w_time))

times
```

# Cluster differences
```{r fig.height=5, fig.width=15}
#plot(clusterward_n, which.plot = 2)
clusterward_n_8 <- cutree(clusterward_n, k = 8)
clusterward_n_8_fac <- factor(clusterward_n_8, labels = paste("Type", 1:8))

clusterward_w_8 <- cutree(clusterward_w, k = 8)
clusterward_w_8_fac <- factor(clusterward_w_8[agg_large$disaggIndex], labels = paste("Type", 1:8))

par(mfrow=c(1,3))
n_plot <- seqIplot(large[,1:10], group = clusterward_n_8_fac, border = NA, use.layout = TRUE, cols = 8, withlegend = F)
w_plot <- seqIplot(large[,1:10], group = clusterward_w_8_fac, border = NA, use.layout = TRUE, cols = 8, withlegend = F)
seqlegend(large)

as.data.frame(clusterward_n_8_fac) %>%
  group_by(clusterward_n_8_fac) %>% summarise(n = n())

as.data.frame(clusterward_w_8_fac) %>%
  group_by(clusterward_w_8_fac) %>% summarise(n = n()) 

```
The first apparent difference is WeightedCluster reorderes the clusters. The clusters look generally the same, though though there is some differing allocation of sequences to cluster membership.  

WeightedCluster splits Type 1 into Type 5 and Type 7. 
Orginially there are two Job Zone 2 clusters, Type 2 and Type 6, but Weighted Cluster puts most of them in Type 8.

WeightedCluster seems to sacrifice some "evenness" in cluster assignment, but it is unclear if that is just particular to this analysis or a trend overall. This seems to be the biggest drawback of WeightedCluster, as the difference in clusters themselves aren't too problematic.

## Documenting large n problem:

In continuing our analysis, we are interested in working with larger sequence objects to gain insight into the career pathways of nonveterans in DC, as well as both veterans and nonveterans in other geographical areas.

Our interest in working with bigger datasets leads us to wonder how capable tools in our current workflow are for handling larger datasets, specifically TraMineR. 

=======

Solution: Use WeightedCluster to aggregate like-sequences before clustering 		http://mephisto.unige.ch/weightedcluster/
		- reduces the number of sequences computed for distance matrix

Solution: Only compute the lower half of the distance matrix ("full.matrix = FALSE")

Solution: Reduce time granluarity from days to months to years etc

Solution: Hidden seqdist optimized OM algorithm ("method = OMopt")

======

Question: 57,000 sequences, Memory error
Solution: "full.matrix = FALSE", WeightedCluster 
https://stackoverflow.com/questions/15929936/problem-with-big-data-during-computation-of-sequence-distances-using-tramine

Question: Sampling to reduce computational time
Solution: Reduce time granularity, hidden option in seqdist() "method = OMopt"
https://stats.stackexchange.com/questions/43540/how-to-randomly-select-5-of-the-sample

Topic: Large n (46,431) limitation on seqdist()
https://r-forge.r-project.org/tracker/index.php?func=detail&aid=6512&group_id=743&atid=2975

Question: Large n (45,000)
Solution: WeightedCluster to aggregate like sequences
http://lists.r-forge.r-project.org/pipermail/traminer-users/2014-November/000256.html

Question: Large event sequence object (long sequence length)
Solution: TraMineR can't handle event sequence objects longer than 3-4
http://lists.r-forge.r-project.org/pipermail/traminer-users/2016-September/000328.html

Question: Large n (25,000) sequence object
Solution: Aggregate identical sequences, lower half of distance matrix, reduce granularity of sequences, use a computer with more RAM
https://stats.stackexchange.com/questions/77130/problem-with-big-data-in-traminer

Question: Discrepancy analysis with TraMineR and aggregated sequence data
Solution:
https://stackoverflow.com/questions/17085780/how-to-use-discrepancy-analysis-with-traminer-and-aggregated-sequence-data