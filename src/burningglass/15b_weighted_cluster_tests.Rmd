---
title: "15b_weighted_cluster_tests"
author: "Joanna Schroeder"
date: "9/4/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
for (pkg in c("tidyverse",  "data.table", "R.utils", "maditr", "stringr", "stringi", "dplyr", "ggplot2", "lubridate", "gt", "DataExplorer", "TraMineR", "TraMineRextras", "cluster", "tools", "WeightedCluster")) {
  library(pkg, character.only = TRUE)
}

uva_color_palette <- 
c("#232D4B", #space cadet
  "#2C4F6B", #indigo dye
  "#0E879C", #blue munsell
  "#60999A", #cadet blue
  "#D1E0BF", #tea green
  "#D9E12B", #pear
  "#E6CE3A", #citrine
  "#E6A01D", #marigold
  "#E57200" #princeton orange
)
```

How does time to run steps of a sequence clustering scale? Two main steps:

1. Calculating a distance matrix
2. Implementing clustering

We will test this using sequence objects with 1,000, 2,000, and 4,000 members. 

## WeightedCluster description

[A brief description of WeightedCluster functionality] (http://mephisto.unige.ch/weightedcluster/)

The main function of WeightedCluster for our purposes is [wcAggregateCases](https://rdrr.io/cran/WeightedCluster/src/R/aggregatecases.R), which aggregates identical cases of a sequence object. The weight of each aggregated case is stored as the AggIndex, and can be retrieved later in analysis (cases can be disaggregated). For example, cases can be aggregated for compuatationally intensive processes like computing distance and clustering, but later disaggregated for visualization.

## Create sequence objects
```{r}
set.seed(09042020)

sts_nonvet <- read.csv("~/git/DSPG2020/career/data/sts_all_ten.csv", row.names = 1)

small <- sts_nonvet[sample(nrow(sts_nonvet), 1000, replace = FALSE, prob = NULL),]
medium <- sts_nonvet[sample(nrow(sts_nonvet), 2000, replace = FALSE, prob = NULL),]
large <- sts_nonvet[sample(nrow(sts_nonvet), 4000, replace = FALSE, prob = NULL),]

small <- seqdef(small, left="DEL", gaps="Civilian unemployment", right="DEL", cpal = c("#CBBEB5",  uva_color_palette[1], uva_color_palette[2],uva_color_palette[4],  uva_color_palette[6], uva_color_palette[9], "#CBBEB5"))

medium <- seqdef(medium, left="DEL", gaps="Civilian unemployment", right="DEL", cpal = c("#CBBEB5",  uva_color_palette[1], uva_color_palette[2],uva_color_palette[4],  uva_color_palette[6], uva_color_palette[9], "#CBBEB5"))

large <- seqdef(large, left="DEL", gaps="Civilian unemployment", right="DEL", cpal = c("#CBBEB5",  uva_color_palette[1], uva_color_palette[2],uva_color_palette[4],  uva_color_palette[6], uva_color_palette[9], "#CBBEB5"))
```
Our sample data comes from a sample of nonveterans who have at least ten years of employment history. We will cluster based on the whole sequence and in exploratory plots limit the axis to the first 30 years of employment history.

## 1,000 member clustering

```{r}
# Normal clustering

reps = 1:10

cluster_times <- for(rep in reps) {
rep = as.character(rep)
  
start_n <- Sys.time()
diss <- seqdist(small, method = "OM", indel = 1, sm = "TRATE", with.missing = TRUE)
clusterward <- agnes(diss, diss = TRUE, method = "ward")
end_n <- Sys.time()
assign(paste0("n_run_", rep), (end_n - start_n), envir = .GlobalEnv)

# Weighted clustering
start_w <- Sys.time()
agg_small <- wcAggregateCases(small)

unique_small <- small[agg_small$aggIndex,]

small.seq <- seqdef(unique_small, weights = agg_small$aggWeights)
diss <- seqdist(small.seq, method = "OM", indel = 1, sm = "TRATE", with.missing = TRUE)
clusterward <- agnes(diss, diss = TRUE, method = "ward")
#clusterward <- hclust(as.dist(diss), method = "ward.D2", members = agg_sts_all_thirty$aggWeights)
end_w <- Sys.time()
assign(paste0("w_run_", rep), (end_w - start_w), envir = .GlobalEnv)
}

small_n_time <- mean.difftime((n_run_1 + n_run_2 + n_run_3 + n_run_4 + n_run_5 + n_run_6 + n_run_7 + n_run_8 + n_run_9 + n_run_10)/10)

small_w_time <- mean.difftime((w_run_1 + w_run_2 + w_run_3 + w_run_4 + w_run_5 + w_run_6 + w_run_7 + w_run_8 + w_run_9 + w_run_10)/10)

small_n_time
small_w_time
```
Weighted clustering is slower. This may be because the sample is so small the extra step of aggregation is costly.

## 2,000 member clustering
```{r}
# Normal clustering

reps = 1:5

cluster_times <- for(rep in reps) {
rep = as.character(rep)
  
start_n <- Sys.time()
diss <- seqdist(medium, method = "OM", indel = 1, sm = "TRATE", with.missing = TRUE)
clusterward <- agnes(diss, diss = TRUE, method = "ward")
end_n <- Sys.time()
assign(paste0("n_run_", rep), (end_n - start_n), envir = .GlobalEnv)

# Weighted clustering
start_w <- Sys.time()
agg_medium <- wcAggregateCases(medium)

unique_medium <- medium[agg_medium$aggIndex,]

medium.seq <- seqdef(unique_medium, weights = agg_medium$aggWeights)
diss <- seqdist(medium.seq, method = "OM", indel = 1, sm = "TRATE", with.missing = TRUE)
clusterward <- agnes(diss, diss = TRUE, method = "ward")
#clusterward <- hclust(as.dist(diss), method = "ward.D2", members = agg_sts_all_thirty$aggWeights)
end_w <- Sys.time()
assign(paste0("w_run_", rep), (end_w - start_w), envir = .GlobalEnv)
}

medium_n_time <- mean.difftime((n_run_1 + n_run_2 + n_run_3 + n_run_4 + n_run_5)/5)

medium_w_time <- mean.difftime((w_run_1 + w_run_2 + w_run_3 + w_run_4 + w_run_5)/5)

medium_n_time
medium_w_time
```
For the medium sample weighted clustering is only slightly faster. Again this is probably due to the fact that aggregating sequences is a costly step.

## 4,000 member clustering
```{r}
# Normal clustering

reps = 2

cluster_times <- for(rep in reps) {
rep = as.character(rep)
  
start_n <- Sys.time()
diss <- seqdist(large, method = "OM", indel = 1, sm = "TRATE", with.missing = TRUE, norm = TRUE)
clusterward_n <- agnes(diss, diss = TRUE, method = "ward")
end_n <- Sys.time()
assign(paste0("n_run_", rep), (end_n - start_n), envir = .GlobalEnv)

# Weighted clustering
start_w <- Sys.time()
agg_large <- wcAggregateCases(large)

unique_large <- large[agg_large$aggIndex,]

large.seq <- seqdef(unique_large, weights = agg_large$aggWeights)
diss <- seqdist(large.seq, method = "OM", indel = 1, sm = "TRATE", with.missing = TRUE, norm = TRUE)
clusterward_w <- agnes(diss, diss = TRUE, method = "ward")
#clusterward <- hclust(as.dist(diss), method = "ward.D2", members = agg_sts_all_thirty$aggWeights)
end_w <- Sys.time()
assign(paste0("w_run_", rep), (end_w - start_w), envir = .GlobalEnv)
assign("clusterward_n", clusterward_n, envir = .GlobalEnv)
assign("clusterward_w", clusterward_w, envir = .GlobalEnv)
}

large_n_time <- mean.difftime((n_run_1 + n_run_2)/2)

large_w_time <- mean.difftime((w_run_1 + w_run_2)/2)

large_n_time
large_w_time
```
For the large sample, the weighted clustering is about twice as fast as the regular clustering. 

## Time table
```{r}
times <- tibble("type" = c("unweighted", "weighted"),
                "1000" = c(small_n_time, small_w_time),
                "2000" = c(medium_n_time, medium_w_time),
                "4000" = c(large_n_time, large_w_time))

times
```
At scale, weighted clustering becomes a more efficient option. Though the "large" samples runs for both methods in around a minute, it is important to remember we are interested in clustering sequence objects with 40,000+ members. 

Assuming no differences in produced clusters, weighted clustering could be a good option to speeding up the clustering process with large samples. There are still other options for speeding up the process though, including faster clustering methods such as PAM and DIANA.

# Cluster differences
```{r fig.height=5, fig.width=15}
#plot(clusterward_n, which.plot = 2)
clusterward_n_8 <- cutree(clusterward_n, k = 4)
clusterward_n_8_fac <- factor(clusterward_n_8, labels = paste("Type", 1:4))

clusterward_w_8 <- cutree(clusterward_w, k = 4)
clusterward_w_8_fac <- factor(clusterward_w_8[agg_large$disaggIndex], labels = paste("Type", 1:4))

#clusterward_w_8_fac <- factor(clusterward_w_8_fac, levels = c("Type 7", "Type 8", "Type 1", "Type 3", "Type 4", "Type 2", "Type 5", "Type 6"))

large$w <- clusterward_w_8[agg_large$disaggIndex]
large$nw <- clusterward_n_8

table(large$nw, large$w)

par(mfrow=c(1,3))
n_plot <- seqIplot(large[,1:30], group = clusterward_n_8_fac, border = NA, use.layout = TRUE, cols = 4, withlegend = F, sortv = "from.start", main = "regular")
w_plot <- seqIplot(large[,1:30], group = clusterward_w_8_fac, border = NA, use.layout = TRUE, cols = 4, withlegend = F, sortv = "from.start", main = "weighted")
seqlegend(large)

as.data.frame(clusterward_n_8_fac) %>%
  group_by(clusterward_n_8_fac) %>% summarise(n = n())

as.data.frame(clusterward_w_8_fac) %>%
  group_by(clusterward_w_8_fac) %>% summarise(n = n()) 


n_plot <- seqIplot(small[,1:30], group = clusterward_nw_8_fac, border = NA, use.layout = TRUE, cols = 8, with.legend = F, sortv = "from.start", main = "regular")
w_plot <- seqIplot(small[,1:30], group = clusterward_w_8_fac, border = NA, use.layout = TRUE, cols = 8, with.legend = F, sortv = "from.start", main = "weighted")
seqlegend(large)
```
Here we compare the cluster outputs from the large sample. Generally, the clusters look the same, though there are a few differences. The most apparent difference is that weighted clustering seems to ignore moreso the normalization parameter for clustering by sequence length. Type 2 and Type 5 clusters are long careers, and Type 6 are all about 15 year careers.

Regular clustering has two missing/unemployment clusters (Type 3 and Type 8), but weighted clustering only has one (Type 1). 

Weighted clustering seems to sacrifice some "evenness" in cluster assignment (range of 1,004 vs 810), but it is unclear if that is just particular to this analysis or a drawback of this method.

Overall, weighted clustering seems like a useful tool for analysis, though should only be used when necessary (for very large n and when seqdist() can't be performed without aggregation).

## Documenting large n problem:

In continuing our analysis, we are interested in working with larger sequence objects to gain insight into the career pathways of nonveterans in DC, as well as both veterans and nonveterans in other geographical areas.

Our interest in working with bigger datasets leads us to wonder how capable tools in our current workflow are for handling larger datasets, specifically TraMineR. 

=======

Solution: Use WeightedCluster to aggregate like-sequences before clustering 		http://mephisto.unige.ch/weightedcluster/
		- reduces the number of sequences computed for distance matrix

Solution: Only compute the lower half of the distance matrix ("full.matrix = FALSE")

Solution: Reduce time granluarity from days to months to years etc

Solution: Hidden seqdist optimized OM algorithm ("method = OMopt")

======

Question: 57,000 sequences, Memory error
Solution: "full.matrix = FALSE", WeightedCluster 
https://stackoverflow.com/questions/15929936/problem-with-big-data-during-computation-of-sequence-distances-using-tramine

Question: Sampling to reduce computational time
Solution: Reduce time granularity, hidden option in seqdist() "method = OMopt"
https://stats.stackexchange.com/questions/43540/how-to-randomly-select-5-of-the-sample

Topic: Large n (46,431) limitation on seqdist()
https://r-forge.r-project.org/tracker/index.php?func=detail&aid=6512&group_id=743&atid=2975

Question: Large n (45,000)
Solution: WeightedCluster to aggregate like sequences
http://lists.r-forge.r-project.org/pipermail/traminer-users/2014-November/000256.html

Question: Large event sequence object (long sequence length)
Solution: TraMineR can't handle event sequence objects longer than 3-4
http://lists.r-forge.r-project.org/pipermail/traminer-users/2016-September/000328.html

Question: Large n (25,000) sequence object
Solution: Aggregate identical sequences, lower half of distance matrix, reduce granularity of sequences, use a computer with more RAM
https://stats.stackexchange.com/questions/77130/problem-with-big-data-in-traminer

Question: Discrepancy analysis with TraMineR and aggregated sequence data
Solution:
https://stackoverflow.com/questions/17085780/how-to-use-discrepancy-analysis-with-traminer-and-aggregated-sequence-data