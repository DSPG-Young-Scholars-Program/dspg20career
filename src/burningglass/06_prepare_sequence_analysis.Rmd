---
title: "06_prepare_sequence_analysis"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
for (pkg in c("tidyverse",  "data.table", "R.utils", "maditr", "stringr", "stringi", "dplyr", "ggplot2", "lubridate", "gt", "DataExplorer", "TraMineR", "TraMineRextras", "cluster")) {
  library(pkg, character.only = TRUE)
}
#double check your working directory
# source("~/git/dspg20career/src/burningglass/03_clean_bgt.R", echo = T, prompt.echo = "", spaced = F)
# #bg_cleaned <- clean_bg(job = job, ed = ed, pers = pers, onet = onet)
# bg_all_demographic <- as.data.table(bg_cleaned$bg_all_demographic)
# bg_vet_demographic <- as.data.table(bg_cleaned$bg_vet_demographic)
# bg_all_job <- as.data.table(bg_cleaned$bg_all_job)
# bg_vet_job <- as.data.table(bg_cleaned$bg_vet_job)
# 
# write.csv(bg_all_demographic, file= "data/01_bg_all_demographic.csv", row.names = F)
# write.csv(bg_vet_demographic, file= "data/02_bg_vet_demographic.csv", row.names = F)
# write.csv(bg_all_job, file= "data/03_bg_all_job.csv", row.names = F)
# write.csv(bg_vet_job, file= "data/04_bg_vet_job.csv", row.names = F)

#bg_all_job <- read.csv("data/03_bg_all_job.csv")
bg_vet_job <- read.csv("data/04_bg_vet_job.csv")
```


```{r eval=FALSE, include=FALSE}
# reconnecting to the database 
# conn <- dbConnect(drv = PostgreSQL(),
#                   dbname = "sdad",
#                   host = "10.250.124.195",
#                   port = 5432,
#                   user = Sys.getenv("db_userid"),
#                   password = Sys.getenv("db_pwd"))
# # writing the new city_clean_finall table to postgis_2
# dbWriteTable(conn, c("bg", "bg_job_dc"), city_clean_final, row.names=F)
# # disconnect from postgresql database
# dbDisconnect(conn)
```


```{r eval=FALSE, include=FALSE}
#select a subset of bg_all_job
# set.seed(1)
# id <- as.vector(bg_all_demographic$id)
# sample_id <-  sample(id, size = 100)
# bg_all_job_sample <- bg_all_job%>%
#   filter(id %in% sample_id)
```

```{r eval=FALSE, include=FALSE}
#dataframe of sample id and veteran indicator
# vet_id <- bg_all_demographic%>%
#   select(id, veteran)
# 
# table(bg_all_job$start_year)
# class(bg_all_job$jobposition)

# first_job <- bg_all_job%>%
#   group_by(id)%>%
#   mutate(max_jobposition = max(jobposition, na.rm=T),  #identify the job that has the maximum of job position, indicates that it's the first job
#          match = if_else(jobposition == max_jobposition, T, F)
#         # today = as.Date("2019-01-01")
#         # days_in_job_market = today - startdate,
#         # days_employed = sum(job_duration_day),
#         # days_unemployed = days_in_job_market-days_employed
#          )%>%
#   filter(max_jobposition >0)%>%
#   filter(match == T)%>%
#   select(id, start_year)%>%
#   rename(first_job_start_year = start_year)

 # last_job <- bg_all_job%>%
 #  group_by(id)%>%
 #  mutate(min_jobposition = min(jobposition, na.rm=T), match = if_else(jobposition == min_jobposition, T, F))%>%
 #  filter(min_jobposition >0)%>%
 #  filter(match == T) %>%
 #  mutate(more_than_one_year_out = if_else(end_year <= 2017, T, F))%>%
 #  select(id, end_year, more_than_one_year_out, noofjobs, onet_job_zone)%>%
 #  rename(last_job_end_year = end_year)

# first_last_job <- first_job %>%
#   left_join(last_job, by = "id")%>%
#    left_join(vet_id, by = "id")%>%
#   mutate(first_job_start_cat = if_else(first_job_start_year < 2000, "before 2000 \n (Seniors)", if_else(first_job_start_year < 2010, "2000-2010 \n (Middle-Aged)", "2010 and later \n (Young Professional)")))%>%
#   mutate(time_in_job_market = 2018- first_job_start_year)

#table(first_last_job$first_job_start_cat)
#table(duplicated(first_last_job$id)) #check duplicates
#table(first_last_job$more_than_one_year_out)
```


#number of jobs
```{r eval=FALSE, include=FALSE}
first_last_job$first_job_start_cat <- factor(first_last_job$first_job_start_cat, levels = c("2010 and later \n (Young Professional)","2000-2010 \n (Middle-Aged)", "before 2000 \n (Seniors)"))

ggplot(first_last_job, aes(x=first_job_start_cat, y=noofjobs, fill = veteran)) + 
  geom_boxplot(outlier.size = 0.1)+
  coord_flip()+
  scale_fill_manual(values=c(uva_color_palette[2], uva_color_palette[8]),  guide = guide_legend(reverse = T))+
   labs(fill="veteran", y = "number of jobs", x="time entering job market")+
  theme_classic() +
    theme(axis.text.y = element_text(size=15),
          axis.title.x= element_text(size=15),
          axis.title.y= element_text(size=15))

```

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# We are choosing years as our unit of analysis because months are unreliable. We are also taking the highest O*NET job zone for a given year to deal with overlapping jobs (jobs have the same start and end date).
bg_veteran_job <- bg_vet_job
bg_vet_job_seq <- bg_vet_job %>%
  select("id", "start_year", "end_year", "onet_job_zone")%>%
  group_by(id)%>%
  arrange(desc(onet_job_zone))%>%
  group_by(id)%>%
  distinct(id, start_year, end_year, .keep_all = TRUE)

# We need an auxillary table with the year of entering the job market to align the sequences
bg_vet_job_first <- bg_vet_job_seq %>% 
  select("id", "start_year") %>% group_by(id) %>% transmute(enter = min(start_year)) %>% distinct() %>% ungroup()

# The seqformat() function does not like any data that has been previously grouped. Here we are "resetting" the data so that it will pass through the function.
bg_vet_job_seq <- as.matrix(bg_vet_job_seq)
bg_vet_job_seq <- as.data.frame(bg_vet_job_seq)
bg_vet_job_first <- as.matrix(bg_vet_job_first)
bg_vet_job_first <- as.data.frame(bg_vet_job_first)


# The input for the function is the prepared sequence table. The data is in format SPELL and we are transforming to format STS. By setting process = TRUE we can align the sequences using the prepared auxillary table.
sts_vet <- seqformat(bg_vet_job_seq, from = "SPELL", to = "STS",
                     id = "id",  begin = "start_year", end = "end_year", 
                     status = "onet_job_zone", process = TRUE,
                     pdata = bg_vet_job_first, pvar = c("id", "enter"))
# Here we are renaming columns to be in format "yn" (year in the job market)
names(sts_vet) <- paste0("y", 1:100)

# sts_vet$id <- seq.int(nrow(sts_vet))
# sts_vet_bc <- as.matrix(sts_vet)
# 
# bc1 <- bclust(sts_vet_bc,
# transformed.par=c(-1.84,-0.99,1.63,0.08,-0.16,-1.68))
```

# Read in Sequence of Veterans

```{r}
sts_vet <- read_csv("~/git/dspg20career/data/sts_vet.csv")%>%
  column_to_rownames("X1")

nrow(sts_vet) #number of veterans
```

```{r}
sts_vet
```

# Create sequence object
```{r}
#delete missing values at front and end
vet.seq <- seqdef(sts_vet, left="DEL", gaps="NA", right="DEL")
class(vet.seq)

#an example of three sequences
vet.seq[1, ]
vet.seq[2, ]
vet.seq[3, ]

#number of matching positions between 2 sequences
seqmpos(vet.seq[1, ], vet.seq[2, ]) 
seqmpos(vet.seq[2, ], vet.seq[3, ])

#length of the longest common prefix (LLCP) between sequence 1 and 2
seqLLCP(vet.seq[1, ], vet.seq[2, ])
seqLLCP(vet.seq[2, ], vet.seq[3, ])

```

# Construct Distance Matrix
```{r}
#LCP (longest common prefix) distance  (distance matrix)
LCP_distance <- 1- seqdist(vet.seq, method = "LCP", norm = TRUE)

#LCS (longest common subsequence) distance (distance matrix)
#treat missing in middle of the sequence as one state
vet.lcs <- seqdist(vet.seq, method = "LCS", with.missing = TRUE)
vet.lcs[1:10, 1:10]
```

# Cost Matrix
```{r}
#substitution-cost matrix
#method="CONSTANT"/"TRATE"
cost_matrix_trate <- seqsubm(vet.seq, method = "TRATE", with.missing = TRUE)
cost_matrix_trate <- round(cost_matrix_trate, 2)
cost_matrix_trate

cost_matrix_constant <- seqsubm(vet.seq, method = "CONSTANT", with.missing = TRUE)
cost_matrix_constant
#optimal matching distance
# vet.seq.OM <- seqdist(vet.seq, method = "OM", sm = cost_matrix_constant)
# vet.seq.OM[1,2]  #optimal distance between sequence 1 and 2 (addition, deletion)

```

# Distance matrix
```{r}
vet.seq.OM <- seqdist(vet.seq, method = "OM", indel = 3, sm = cost_matrix_trate, with.missing = TRUE)

#The necessary size to store the matrix is roughly 453.5 Mb
object.size(vet.seq.OM)/1024^2

#glimpse of the distance matrix
round(vet.seq.OM[1:10, 1:10], 1)
```

# Clustering
```{r}
#clustering
# start_time <- Sys.time()
# clusterward <- agnes(vet.seq.OM, diss = TRUE, method = "ward")
# end_time <- Sys.time()
#saveRDS(clusterward, file = "data/clusterward.rds")
#message("run time: ", end_time - start_time)


clusterward <- readRDS(file = "/sfs/qumulo/qhome/zz3hs/git/dspg20career/data/clusterward.rds")

#dendrogram 
plot(clusterward, which.plots =2)

cluster3 <- cutree(clusterward, k=3)
cluster3 <- factor(cluster3, labels = c("Type 1",  "Type 2", "Type 3"))
table(cluster3)

#longitudinal plot
seqfplot(vet.seq, group = cluster3, pbarw = T)

#another cluster plot
seqmtplot(vet.seq, group = cluster3)
```

#Bayesian Hierarchical Clustering Using Spike and Slab Models
Designed for low-sample-size-high-dimensional situations
```{r}
library(bclust)
data(gaelle)

# unreplicated clustering
gaelle.bclust<-bclust(x=gaelle,
transformed.par=c(-1.84,-0.99,1.63,0.08,-0.16,-1.68)) 

par(mfrow=c(2,1))
plot(as.dendrogram(gaelle.bclust))
abline(h=gaelle.bclust$cut)
plot(gaelle.bclust$clust.number,gaelle.bclust$logposterior,
xlab="Number of clusters",ylab="Log posterior",type="b")
abline(h=max(gaelle.bclust$logposterior))

#replicated clustering
gaelle.id<-rep(1:14,c(3,rep(4,13))) 
# first 3 rows replication of ColWT 
# 4 replications for the others

gaelle.lab<-c("ColWT","d172","d263","isa2",
"sex4","dpe2","mex1","sex3","pgm","sex1",
"WsWT","tpt","RLDWT","ke103")

gaelle.bclust<-bclust(gaelle,rep.id=gaelle.id,labels=gaelle.lab,
transformed.par=c(-1.84,-0.99,1.63,0.08,-0.16,-1.68))
plot(as.dendrogram(gaelle.bclust))
abline(h=gaelle.bclust$cut)
plot(gaelle.bclust$clust.number,gaelle.bclust$logposterior,
xlab="Number of clusters",ylab="Log posterior",type="b")
abline(h=max(gaelle.bclust$logposterior))
```

